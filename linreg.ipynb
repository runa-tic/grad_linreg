{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQjq1AU49Q-q"
   },
   "source": [
    "# Домашнее задание №1: линейная регрессия (максимум 10 баллов)\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1FsuljEb9Q-w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My variant is 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('My variant is', len('Nurullin')%4+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChBks_AF9Q-y"
   },
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jihDOyOd9Q-0"
   },
   "source": [
    "Создадим набор данных для многомерной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ykkouQff9Q-2",
    "outputId": "8223f891-c775-432e-c125-53bff94c60c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZP4pwT9Q-3"
   },
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a-cl2ujU9Q-4",
    "outputId": "dc936815-cd0a-4c42-bc79-824337410051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3816291655653714e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IKaW67f9Q-6"
   },
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Fzw6etrW9Q-7",
    "outputId": "9a1effb5-b3d9-47cc-bd8b-c696505c8521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3043486091961774e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.34729853e-08, -4.29843662e-08,  3.98497322e-08,  5.89662903e-08,\n",
       "        2.90767650e+01,  3.18894632e-08, -3.99833392e-08, -2.69441716e-09,\n",
       "        6.94140069e+01, -2.64269448e-08,  1.34698853e-08, -8.61501732e-09,\n",
       "       -2.81073594e-09,  7.64336917e+01, -3.34615737e-08,  6.11485473e-09,\n",
       "        3.04716044e-08,  7.99649209e-08,  1.93735605e-09,  2.85389122e-08,\n",
       "       -3.38181107e-08, -4.80362379e-09, -2.16407343e-08,  1.40467593e-08,\n",
       "        4.96399051e-08, -2.35903552e-08,  2.62286479e-08, -7.08143490e-09,\n",
       "        2.60277106e-08,  2.11942325e-08,  6.45710686e-08, -3.81760643e-08,\n",
       "       -4.62978761e-08, -1.14285137e-08, -1.04349721e-08, -4.29251684e-08,\n",
       "       -8.28129539e-08, -5.76639177e-08,  8.74741503e-09,  1.39972416e-08,\n",
       "       -6.04385519e-09, -5.14225428e-09, -3.52367299e-08,  6.34308899e+01,\n",
       "       -3.05487861e-09, -1.14381906e-08, -8.12381902e-09, -3.45311210e-08,\n",
       "       -3.11557823e-09,  1.82319081e-08,  7.58480855e-09,  1.81336847e-09,\n",
       "        1.19081085e-09, -9.76025551e-10,  3.04371411e+01,  4.86414791e-08,\n",
       "       -3.24538474e-08,  7.83398775e+01,  4.68412055e-08,  3.21919212e-08,\n",
       "        6.91908143e+01,  6.72548390e-08, -2.32466186e-08,  6.26599789e-08,\n",
       "        1.14044956e-09, -6.41508340e-08, -7.01519539e-08, -1.54032876e-08,\n",
       "        8.15845558e+01, -8.04013340e-08,  3.99144480e-10, -5.17046596e-08,\n",
       "        1.70493649e-08, -1.77414084e-08,  2.78181333e-08, -5.31047043e-08,\n",
       "        3.86503264e+01, -4.25672130e-08, -3.95797806e-08, -3.25042075e-08,\n",
       "        3.86173114e-08,  1.33258626e-08, -6.03029766e-08, -4.65431012e-08,\n",
       "        1.91832676e+01, -4.96374869e-08, -1.07329714e-08, -7.65191522e-08,\n",
       "        1.41282816e-08,  3.99415669e-08,  3.43992994e-08, -5.42656061e-08,\n",
       "        4.07512586e-08,  3.44213140e-08, -3.72769010e-10, -3.86245355e-09,\n",
       "        2.42059765e-08,  4.50403375e-08,  2.04997647e-08, -2.23583216e-08])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq1076z-9Q-8"
   },
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) LinearRegression() находит решение линейной регрессии аналитически, а стохастический градиент - \"методом тыка\", поэтому логично, что ошибка аналитической линейной регрессии будет на несколько порядков ниже ошибки стохастического метода.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Есть два способа улучшения качества модели. Первое - понижение альфы. Чем меньше alpha, тем меньше штраф (регуляризация), что позволяет модели лучше адаптироваться к данным (тут главное не переобучить). Второе - увеличение количества доступных итераций в функци SGDRegressor\n",
    "\n",
    "Однако стоит отметить, что очень маленькая alpha может привести к медленному обучению. Баланс между alpha и количеством итераций — ключ к хорошей модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.607270071553692e-25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Пусть альфа равна 1e-20 - на 9 порядков меньше альфы из примера выше\n",
    "\n",
    "reg = SGDRegressor(alpha=1e-20, max_iter=100000000).fit(X, y)\n",
    "\n",
    "mean_squared_error(y, reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jJplHqS9Q-9"
   },
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKeXXhEH9Q--"
   },
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "VY3CTs0W9Q-_"
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape   #(10000, 100)\n",
    "        \n",
    "        self.weights = np.zeros(n_features) #задаем начальные веса равными 0\n",
    "        self.bias = 0                       #Начальное смещение = 0\n",
    "        \n",
    "        prev_weights = np.zeros(n_features) #Предыдущие веса для отслеживания изменений\n",
    "        prev_mse = 1e+20              # Предыдущее значение MSE - на шару что нибудь большое влепил\n",
    "        \n",
    "        for iteration in range(self.max_iter):   #1000 итераций\n",
    "            y_pred = np.dot(X, self.weights) + self.bias #получаем вектор предсказаний длиной n_samples\n",
    "               \n",
    "            errors = y_pred - y #Расчет ошибок\n",
    "            #расчет grad - векторов\n",
    "            grad_weight = np.dot(X.T, errors) / n_samples\n",
    "            grad_bias = np.sum(errors) / n_samples\n",
    "            \n",
    "            #L1 регуляризация\n",
    "            grad_weight += self.l_ratio * np.sign(self.weights)\n",
    "            \n",
    "            self.weights -= self.alpha * grad_weight\n",
    "            self.bias -= self.alpha * grad_bias\n",
    "            \n",
    "            mse = np.sum(errors**2) / n_samples\n",
    "            \n",
    "            #Критерий остановки\n",
    "            weight_diff = np.linalg.norm(self.weights - prev_weights)\n",
    "            \n",
    "            if weight_diff < self.tol:\n",
    "                print('ура ура сошлось')\n",
    "                break\n",
    "            \n",
    "            # Обновление предыдущих значений\n",
    "            prev_weights = np.copy(self.weights)\n",
    "            prev_mse = mse\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "vH6XOYJm9Q-_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 29487.950583963997\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y, my_reg.predict(X))\n",
    "print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается какая то беда, условие останова даже не выполняется, попробуем взять скорость обучения побольше (также можно сделать больше итераций)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ура ура сошлось\n",
      "MSE: 1.45130184084742e-05\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(alpha=0.1)\n",
    "my_reg.fit(X, y)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y, my_reg.predict(X))\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!') \n",
    "#more like it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBRR_3Sh9Q_A"
   },
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Lasso (sklearn): 0.10082940910578796\n",
      "MSE for Lasso (DIY variant): 29487.950583963997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "y_pred_lasso = lasso_reg.predict(X)\n",
    "mse_lasso = mean_squared_error(y, y_pred_lasso)\n",
    "print(f'MSE for Lasso (sklearn): {mse_lasso}')\n",
    "\n",
    "my_lasso_reg = LinearRegression()\n",
    "my_lasso_reg.fit(X, y)\n",
    "\n",
    "y_pred_my_lasso = my_lasso_reg.predict(X)\n",
    "mse_my_lasso = mean_squared_error(y, y_pred_my_lasso)\n",
    "print(f'MSE for Lasso (DIY variant): {mse_my_lasso}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ура ура сошлось\n",
      "MSE for Lasso (sklearn): 0.10082940910578796\n",
      "MSE for Lasso (DIY variant): 0.010705266376535039\n"
     ]
    }
   ],
   "source": [
    "my_lasso_reg = LinearRegression(alpha=0.01)\n",
    "my_lasso_reg.fit(X, y)\n",
    "\n",
    "y_pred_my_lasso = my_lasso_reg.predict(X)\n",
    "mse_my_lasso = mean_squared_error(y, y_pred_my_lasso)\n",
    "print(f'MSE for Lasso (sklearn): {mse_lasso}')\n",
    "print(f'MSE for Lasso (DIY variant): {mse_my_lasso}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в общем, выбранное значение learing rate как будто бы не очень хорошо предсказывает, поэтому я взял alpha=0.01, и получил результаты луше \n",
    "\n",
    "судя по результатам, сам sklearn использует значение альфа примерно равное 0.01\n",
    "\n",
    "В то же время, если взять alpha=0.1 (см. пункт выше), результат мсе получится еще меньше -> выше качество.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
